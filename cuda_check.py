import subprocess
import sys
import importlib.util
import torch

def check_command(cmd):
    try:
        result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True)
        print(f"\n[âœ…] å‘½ä»¤ `{cmd}` æ‰§è¡ŒæˆåŠŸ:\n{result.stdout}")
    except subprocess.CalledProcessError as e:
        print(f"\n[âŒ] å‘½ä»¤ `{cmd}` æ‰§è¡Œå¤±è´¥:\n{e.stderr}")

def check_module(module_name):
    return importlib.util.find_spec(module_name) is not None

def check_pytorch():
    print("\nğŸ§ª PyTorch æ£€æŸ¥ï¼š")
    if check_module("torch"):
        try:
            import torch
            print(f"[âœ…] å·²å®‰è£… torchï¼Œç‰ˆæœ¬ï¼š{torch.__version__}")
            print(torch.cuda.is_available())
            print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No CUDA")
        except Exception as e:
            print(f"[âŒ] æ£€æŸ¥ torch å‡ºé”™: {e}")
    else:
        print("[âŒ] æœªå®‰è£… PyTorchï¼ˆtorchï¼‰æ¨¡å—")

def check_tensorflow():
    print("\nğŸ§ª TensorFlow æ£€æŸ¥ï¼š")

    if check_module("tensorflow"):
        try:
            import tensorflow as tf
            print(f"[âœ…] å·²å®‰è£… TensorFlowï¼Œç‰ˆæœ¬ï¼š{tf.__version__}")
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                print(f"[âœ…] æ£€æµ‹åˆ° GPUï¼š{gpus}")
            else:
                print("[âŒ] æœªæ£€æµ‹åˆ° GPUï¼Œè¯·æ£€æŸ¥ CUDA é©±åŠ¨æˆ–å®‰è£…æ˜¯å¦ä¸º GPU ç‰ˆæœ¬")
        except Exception as e:
            print(f"[âŒ] æ£€æŸ¥ tensorflow å‡ºé”™: {e}")
    else:
        print("[âŒ] æœªå®‰è£… TensorFlow æ¨¡å—")

def main():
    print("==== CUDA / é©±åŠ¨ / æ·±åº¦å­¦ä¹ ç¯å¢ƒ æ£€æŸ¥å·¥å…· ====")
    check_command("nvidia-smi")
    check_pytorch()
    check_tensorflow()

if __name__ == "__main__":
    main()
