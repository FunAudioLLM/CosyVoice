import os
import sys
import argparse
import gradio as gr
import numpy as np
import torch
import torchaudio
import random
import librosa

ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append('{}/third_party/Matcha-TTS'.format(ROOT_DIR))

from cosyvoice.cli.cosyvoice import CosyVoice
from cosyvoice.utils.file_utils import load_wav, logging
from cosyvoice.utils.common import set_all_random_seed

inference_mode_list = ['Cross-lingual replication', '3-second fast replication']
# inference_mode_list = ['Pre-trained voice', '3-second fast replication', 'Cross-lingual replication', 'Natural language control']
instruct_dict = {
    # 'Pre-trained voice': '1. Select pre-trained voice\n2. Click the generate audio button',
    '3-second fast replication': '1. Select the prompt audio file, or record the prompt audio, ensuring it does not exceed 30 seconds. If both are provided, the prompt audio file will be prioritized\n2. Enter the prompt text\n3. Click the generate audio button',
    'Cross-lingual replication': '1. Select the prompt audio file, or record the prompt audio, ensuring it does not exceed 30 seconds. If both are provided, the prompt audio file will be prioritized\n2. Click the generate audio button',
    # 'Natural language control': '1. Select pre-trained voice\n2. Enter the instruct text\n3. Click the generate audio button'
}
examples = [
    [
        "Please enter the text to be synthesized, select the inference mode, and follow the instruction steps. BÃ¢y giá», em chá»‰ cáº§n biáº¿t bÃ¡n kÃ­nh r cá»§a hÃ¬nh trÃ²n lÃ  bao nhiÃªu, rá»“i thay vÃ o cÃ´ng thá»©c trÃªn lÃ  ra ngay diá»‡n tÃ­ch thÃ´i. Em thá»­ tÃ­nh xem nÃ o! Náº¿u cáº§n giÃºp thÃªm bÆ°á»›c nÃ o, cá»© há»i anh nha!",
        "Cross-lingual replication",
        "náº¿u cÃ¡c cáº­u muá»‘n váº­y, Ä‘Ã¢y lÃ  báº£o má»‘i dÃ nh cho tráº» em. ChÃºng lÃ  má»™t Ä‘á»“ chÆ¡i Ä‘á»ƒ bay ra ngoÃ i vÅ© trá»¥ trong tÆ°Æ¡ng lai Ã¡. Äáº§u tiÃªn lÃ  báº£o bá»‘i tÃªn lá»­a cáº¥m tráº¡i ngoÃ i vÅ© trá»¥.",
        "samples/doremon.mp3"
    ],
    [
        "Please enter the text to be synthesized, select the inference mode, and follow the instruction steps. BÃ¢y giá», em chá»‰ cáº§n biáº¿t bÃ¡n kÃ­nh r cá»§a hÃ¬nh trÃ²n lÃ  bao nhiÃªu, rá»“i thay vÃ o cÃ´ng thá»©c trÃªn lÃ  ra ngay diá»‡n tÃ­ch thÃ´i. Em thá»­ tÃ­nh xem nÃ o! Náº¿u cáº§n giÃºp thÃªm bÆ°á»›c nÃ o, cá»© há»i anh nha!",
        "Cross-lingual replication",
        "CÃ¡c báº¡n Ä‘Ã£ bao giá» gáº·p pháº£i cáº£m giÃ¡c vá»™i dá»‘i khi pháº£i Ä‘á»‘i máº·t vá»›i má»™t cuá»™c sá»‘ng hoÃ n toÃ n má»›i mÃ  khÃ´ng ai nháº¯c nhá»Ÿ hay hÆ°á»›ng dáº«n tá»«ng bÆ°á»›c, hay cáº£m tháº¥y Ã¡p lá»±c náº·ng ná» tá»« viá»‡c há»c táº­p khiáº¿n cho báº¡n dá»… dÃ ng máº¥t Ä‘i Ä‘á»‹nh hÆ°á»›ng ngay tá»« nhá»¯ng ngÃ y Ä‘áº§u há»c Ä‘áº¡i há»c? á»ž Ä‘áº¡i há»c, sáº½ khÃ´ng cÃ²n ai giÃ¡m sÃ¡t hay nháº¯c nhá»Ÿ báº¡n nhÆ° thá»i trung há»c ná»¯a.",
        "samples/quynh.wav"
    ],
    [
        "Please enter the text to be synthesized, select the inference mode, and follow the instruction steps. BÃ¢y giá», em chá»‰ cáº§n biáº¿t bÃ¡n kÃ­nh r cá»§a hÃ¬nh trÃ²n lÃ  bao nhiÃªu, rá»“i thay vÃ o cÃ´ng thá»©c trÃªn lÃ  ra ngay diá»‡n tÃ­ch thÃ´i. Em thá»­ tÃ­nh xem nÃ o! Náº¿u cáº§n giÃºp thÃªm bÆ°á»›c nÃ o, cá»© há»i anh nha!",
        "Cross-lingual replication",
        "NgÃ y hÃ´m nay ngá»“i Ä‘Ã¢y vá»›i chÃºng ta khÃ´ng chá»‰ lÃ  má»™t SÆ¡n TÃ¹ng ca sÄ©, nháº¡c sÄ© mÃ  cÃ¡ch Ä‘Ã¢y ba bá»‘n thÃ¡ng thÃ¬ báº¡n áº¥y cÃ²n vá»«a Ä‘áº£m nhiá»‡m má»™t vai trÃ² má»›i lÃ  giÃ¡m Ä‘á»‘c cÃ´ng ty giáº£i trÃ­ mang tÃªn mÃ¬nh. Vai trÃ² má»›i, niá»m vui má»›i nhÆ°ng cháº¯c cháº¯n trÃ¡ch nhiá»‡m vÃ  Ã¡p lá»±c cÅ©ng má»›i Ä‘Ãºng khÃ´ng?",
        "samples/diep-chi.wav"
    ],
    [
        "Please enter the text to be synthesized, select the inference mode, and follow the instruction steps. BÃ¢y giá», em chá»‰ cáº§n biáº¿t bÃ¡n kÃ­nh r cá»§a hÃ¬nh trÃ²n lÃ  bao nhiÃªu, rá»“i thay vÃ o cÃ´ng thá»©c trÃªn lÃ  ra ngay diá»‡n tÃ­ch thÃ´i. Em thá»­ tÃ­nh xem nÃ o! Náº¿u cáº§n giÃºp thÃªm bÆ°á»›c nÃ o, cá»© há»i anh nha!",
        "Cross-lingual replication",
        "Xin chÃ o, tÃ´i lÃ  má»™t trá»£ lÃ½ AI cÃ³ kháº£ nÄƒng trÃ² chuyá»‡n vá»›i báº¡n báº±ng giá»ng nÃ³i tá»± nhiÃªn, Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi nhÃ³m NÃ³n LÃ¡. TÃ´i cÃ³ thá»ƒ há»— trá»£ ngÆ°á»i khiáº¿m thá»‹, Ä‘á»c sÃ¡ch nÃ³i, lÃ m trá»£ lÃ½ áº£o, review phim, lÃ m waifu Ä‘á»ƒ an á»§i báº¡n vÃ  phá»¥c vá»¥ nhiá»u má»¥c Ä‘Ã­ch khÃ¡c.",
        "samples/nu-nhe-nhang.wav"
    ],
    [
        "Please enter the text to be synthesized, select the inference mode, and follow the instruction steps. BÃ¢y giá», em chá»‰ cáº§n biáº¿t bÃ¡n kÃ­nh r cá»§a hÃ¬nh trÃ²n lÃ  bao nhiÃªu, rá»“i thay vÃ o cÃ´ng thá»©c trÃªn lÃ  ra ngay diá»‡n tÃ­ch thÃ´i. Em thá»­ tÃ­nh xem nÃ o! Náº¿u cáº§n giÃºp thÃªm bÆ°á»›c nÃ o, cá»© há»i anh nha!",
        "Cross-lingual replication",
        "Äáº©y máº¡nh cÃ´ng tÃ¡c thÃ´ng tin tuyÃªn truyá»n, ká»‹p thá»i thÃ´ng tin, cung cáº¥p cÃ¡c thÃ´ng tin trinh thá»‘ng Ä‘áº¿n táº¥t cáº£ cÃ¡c Ä‘Æ¡n vá»‹ trÃªn cáº£ nÆ°á»›c. ÄÃ¢y lÃ  tÃ´i Ä‘ang nÃ³i nhá»¯ng ná»™i dung mÃ  nÃ³ khÃ´ng cÃ³ Ã½ nghÄ©a má»¥c Ä‘Ã­ch lÃ  test há»‡ thá»‘ng cá»§a Tech Channel",
        "samples/atuan.wav"
    ],
    [
        "Please enter the text to be synthesized, select the inference mode, and follow the instruction steps. BÃ¢y giá», em chá»‰ cáº§n biáº¿t bÃ¡n kÃ­nh r cá»§a hÃ¬nh trÃ²n lÃ  bao nhiÃªu, rá»“i thay vÃ o cÃ´ng thá»©c trÃªn lÃ  ra ngay diá»‡n tÃ­ch thÃ´i. Em thá»­ tÃ­nh xem nÃ o! Náº¿u cáº§n giÃºp thÃªm bÆ°á»›c nÃ o, cá»© há»i anh nha!",
        "Cross-lingual replication",
        "Äiá»ƒm thá»© hai mÃ  chÃºng tÃ´i cÅ©ng xin thÆ°a quÃ½ vá»‹ lÃ  cÅ©ng trong cÃ¡i buá»•i nÃ³i chuyá»‡n Ä‘Ã³ thÃ¬ tÃ´i cÃ³ Ä‘á» cáº­p tá»›i má»™t phá»¥ ná»¯ Viá»‡t Nam Ä‘Ã£ sá»‘ng táº¡i Má»¹ 17 nÄƒm mÃ  bá»‹ trá»¥c xuáº¥t ThÆ°a quÃ½ vá»‹ báº£n tin nÃ y chÃºng tÃ´i cÅ©ng Ä‘á»c á»Ÿ trÃªn bÃ¡o hoáº·c lÃ  trÃªn máº¡ng nhÆ°ng mÃ  khÃ´ng cÃ³ kiá»ƒm chá»©ng chi tiáº¿t cho chÃ­nh xÃ¡c trÆ°á»›c khi lÃªn Ä‘Æ°á»ng qua Ã‚u Chá»™c thÃ¬ chÃºng tÃ´i xin rÃºt láº¡i cÃ¢u chuyá»‡n nÃ y Ä‘á»ƒ trÃ¡nh sá»± hiá»ƒu láº§m vá» chÃ­nh sÃ¡ch di trÃº cá»§a Hoa Ká»‹ch. ThÆ°a quÃ½ vá»‹",
        "samples/nguyen-ngoc-ngan.wav"
    ],
    [
        "Please enter the text to be synthesized, select the inference mode, and follow the instruction steps. BÃ¢y giá», em chá»‰ cáº§n biáº¿t bÃ¡n kÃ­nh r cá»§a hÃ¬nh trÃ²n lÃ  bao nhiÃªu, rá»“i thay vÃ o cÃ´ng thá»©c trÃªn lÃ  ra ngay diá»‡n tÃ­ch thÃ´i. Em thá»­ tÃ­nh xem nÃ o! Náº¿u cáº§n giÃºp thÃªm bÆ°á»›c nÃ o, cá»© há»i anh nha!",
        "Cross-lingual replication",
        "å¸Œæœ›ä½ ä»¥åŽèƒ½å¤Ÿåšå¾—æ¯”æˆ‘è¿˜å¥½å“Ÿ",
        "samples/zero_shot_prompt.wav"
    ]
]
stream_mode_list = [('No', False), ('Yes', True)]
max_val = 0.8

def generate_seed():
    seed = random.randint(1, 100000000)
    return {
        "__type__": "update",
        "value": seed
    }

def postprocess(speech, top_db=60, hop_length=220, win_length=440):
    speech, _ = librosa.effects.trim(
        speech, top_db=top_db,
        frame_length=win_length,
        hop_length=hop_length
    )
    if speech.abs().max() > max_val:
        speech = speech / speech.abs().max() * max_val
    speech = torch.concat([speech, torch.zeros(1, int(target_sr * 0.2))], dim=1)
    return speech

def change_instruction(mode_checkbox_group):
    return instruct_dict[mode_checkbox_group]

def generate_audio(
    tts_text, mode_checkbox_group,
    prompt_text, prompt_wav_upload, prompt_wav_record,
    seed, stream, speed
):
    if prompt_wav_upload is not None:
        prompt_wav = prompt_wav_upload
    elif prompt_wav_record is not None:
        prompt_wav = prompt_wav_record
    else:
        prompt_wav = None

    # Natural Language Control Mode
    if mode_checkbox_group in ['Natural language control']:
        if cosyvoice.frontend.instruct is False:
            gr.Warning('You are using natural language control mode, the {} model does not support this mode, please use the iic/CosyVoice-300M-Instruct model.'.format(args.model_dir))
            yield (target_sr, default_data)
        if prompt_wav is not None or prompt_text != '':
            gr.Info('You are using natural language control mode, the prompt audio/prompt text will be ignored.')

    # Cross-lingual Mode
    if mode_checkbox_group in ['Cross-lingual replication']:
        if cosyvoice.frontend.instruct is True:
            gr.Warning('You are using cross-lingual replication mode, the {} model does not support this mode, please use the iic/CosyVoice-300M model.'.format(args.model_dir))
            yield (target_sr, default_data)
        if prompt_wav is None:
            gr.Warning('You are using cross-lingual replication mode, please provide the prompt audio.')
            yield (target_sr, default_data)

    # 3-second Fast Replication Mode
    if mode_checkbox_group in ['3-second fast replication', 'Cross-lingual replication']:
        if prompt_wav is None:
            gr.Warning('Audio reference must not be empty.')
            yield (target_sr, default_data)
        if torchaudio.info(prompt_wav).sample_rate < prompt_sr:
            gr.Warning('Audio reference sampling rate {} is lower than {}.'.format(torchaudio.info(prompt_wav).sample_rate, prompt_sr))
            yield (target_sr, default_data)

    # Pre-trained Voice Mode
    # 3-second Fast Replication Mode
    if mode_checkbox_group in ['3-second fast replication']:
        if prompt_text == '':
            gr.Warning('Reference text must not be empty.')
            yield (target_sr, default_data)

    if mode_checkbox_group == 'Pre-trained voice':
        logging.info('Get SFT inference request')
        set_all_random_seed(seed)
        for i in cosyvoice.inference_sft(tts_text, stream=stream, speed=speed):
            yield (target_sr, i['tts_speech'].numpy().flatten())
    elif mode_checkbox_group == '3-second fast replication':
        logging.info('Get zero-shot inference request')
        prompt_speech_16k = postprocess(load_wav(prompt_wav, prompt_sr))
        set_all_random_seed(seed)
        for i in cosyvoice.inference_zero_shot(tts_text, prompt_text, prompt_speech_16k, stream=stream, speed=speed):
            yield (target_sr, i['tts_speech'].numpy().flatten())
    elif mode_checkbox_group == 'Cross-lingual replication':
        logging.info('Get cross-lingual inference request')
        prompt_speech_16k = postprocess(load_wav(prompt_wav, prompt_sr))
        set_all_random_seed(seed)
        for i in cosyvoice.inference_cross_lingual(tts_text, prompt_speech_16k, stream=stream, speed=speed):
            yield (target_sr, i['tts_speech'].numpy().flatten())
    else:
        logging.info('Get instruct inference request')
        set_all_random_seed(seed)
        for i in cosyvoice.inference_instruct(tts_text,None, stream=stream, speed=speed):
            yield (target_sr, i['tts_speech'].numpy().flatten())

def main():
    with gr.Blocks() as demo:
        gr.Markdown("# ðŸµ CosyVoice: A fast TTS architecture with conditional flow matching")
        with gr.Row():
            with gr.Column():
                tts_text = gr.Textbox(
                    label="Input text",
                    lines=5,
                    value="Please enter the text to be synthesized, select the inference mode, and follow the instruction steps. BÃ¢y giá», em chá»‰ cáº§n biáº¿t bÃ¡n kÃ­nh r cá»§a hÃ¬nh trÃ²n lÃ  bao nhiÃªu, rá»“i thay vÃ o cÃ´ng thá»©c trÃªn lÃ  ra ngay diá»‡n tÃ­ch thÃ´i. Em thá»­ tÃ­nh xem nÃ o! Náº¿u cáº§n giÃºp thÃªm bÆ°á»›c nÃ o, cá»© há»i anh nha! "
                )
                with gr.Tabs(elem_id="prompt_wav_tabs"):
                    with gr.TabItem("Upload"):
                        prompt_wav_upload = gr.Audio(sources='upload', type='filepath', label='Reference audio file')
                    with gr.TabItem("Record"):
                        prompt_wav_record = gr.Audio(sources='microphone', type='filepath', label='Record prompt audio file')

                prompt_text = gr.Textbox(label="Reference text", lines=5)
                mode_checkbox_group = gr.Radio(
                    choices=inference_mode_list,
                    label='Mode',
                    value=inference_mode_list[0]
                )
                stream = gr.Radio(
                    label='Enable streaming inference',
                    choices=stream_mode_list,
                    value=stream_mode_list[0][1],
                    visible=False
                )
                speed = gr.Slider(label="Speed", value=1, minimum=0.5, maximum=2.0, step=0.1)
                seed_button = gr.Button(value="\U0001F3B2", visible=False)
                seed = gr.Number(value=0, label="Random inference seed", visible=False)
            with gr.Column():
                output_audio = gr.Audio(label='Generated audio')
                generate_btn = gr.Button("Generate", variant="primary")
                gr.Examples(examples, inputs=[tts_text, mode_checkbox_group, prompt_text, prompt_wav_upload])

            seed_button.click(fn=generate_seed, outputs=seed)
            generate_btn.click(
                fn=generate_audio,
                inputs=[
                    tts_text, mode_checkbox_group,
                    prompt_text, prompt_wav_upload,
                    prompt_wav_record, seed,
                    stream, speed
                ],
                outputs=output_audio
            )
    demo.title = 'ðŸµ CosyVoice: A fast TTS architecture with conditional flow matching'
    demo.queue(max_size=4, default_concurrency_limit=2)
    demo.launch(server_name='0.0.0.0', server_port=args.port)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--port',
                        type=int,
                        default=8000)
    parser.add_argument('--model_dir',
                        type=str,
                        default='pretrained_models/CosyVoice-300M',
                        help='local path or modelscope repo id')
    args = parser.parse_args()
    cosyvoice = CosyVoice(args.model_dir)
    sft_spk = cosyvoice.list_avaliable_spks()
    prompt_sr, target_sr = 16000, 22050
    default_data = np.zeros(target_sr)
    main()